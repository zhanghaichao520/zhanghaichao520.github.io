---
layout:     post
title:      Hadoop入门
subtitle:   惜我十年前，与君始相识
date:       2018-09-10
author:     zhanghaichao
header-img: img/post-bg-ioses.jpg
catalog: true
tags:
    - Hadoop
    - 大数据   
---

## 一 初识Hadoop
 1. 分布式文件系统-HDFS
    
 2. 资源调度系统-YARN
 	![](https://ws2.sinaimg.cn/large/0069RVTdly1fv47hrpdw4j30rv0ehq5b.jpg)
 	
 3. 分布式计算框架-MapReduce
 	![](https://ws2.sinaimg.cn/large/0069RVTdly1fv47r5j0chj30u40e240p.jpg)
 	
 4. 优势
    - 高可靠性
    - 高扩展性
    - 其他：存储在廉价机器；成熟的生态圈 	 
   
 5. 生态系统
 
 	![](https://ws1.sinaimg.cn/large/0069RVTdly1fv48jbpzdbj30k10ac0sz.jpg)
 	
 6. 常用发行版及选型
    - apache hadoop
    - CDH：不开源，安装方便，jar冲突较少。https://archive.cloudera.com/cdh5/cdh/5/
    - HDP：原装hadoop，开源


## 二 分布式文件系统HDFS
 1. HDFS概述.  
    - 源于谷歌2003年论文GFS的开源实现。
    - 设计目标：非常巨大的分布式文件系统；运行在普通廉价的硬件上；易扩展，为用户提供不错的文件存储服务
 2. HDFS架构.    
    - Client：切分文件；访问HDFS；与NameNode交互，获取文件位置信息；与DataNode交互，读取和写入数据。
    - NameNode：Master节点，负责客户端请求的响应；负责元数据（文件名称、副本系数、block存档的datanode）的管理
    - DataNode：Slave节点，存储实际的数据，汇报存储信息给NameNode。
    - Secondary NameNode：辅助NameNode，分担其工作量；定期合并fsimage和fsedits，推送给NameNode；紧急情况下，可辅助恢复NameNode，但Secondary NameNode并非NameNode的热备。
    - NameNode+N个DataNode：最好部署在不同的节点上。  
 3. HDFS副本机制.  
    ![](https://ws2.sinaimg.cn/large/0069RVTdgy1fv4a4c6zr5j30or05wjse.jpg)
    副本存放策略：默认3个副本，第一个节点存在当前操作客户端机架上，另外两个节点存在另一台机架上 
 4. HDFS环境搭建.  
    - 需要预先装的软件：java（jdk，配置环境变量）、[SSH](https://zhanghaichao520.github.io/2018/09/08/%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8Blinux/#1ssh%E5%91%BD%E4%BB%A4)
    - 解压hadoop安装包：   
    
		```
		tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz -C ~/hadoop
		```   
		
	- 配置文件修改（～/hadoop/etc/hadoop/):    
    	- hadoop-env.sh:加入JAVA_HOME环境.  
    	  
    	```
    	export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_171.jdk/Contents/Home
    	```  
    	
    	- core-site.xml
    	 
		```  
     <property>
            <name>fs.defaultFS</name>
            <value>hdfs://hadoop-1:9000</value>
     </property>
     <property>
         	  <name>io.file.buffer.size</name>
            <value>131072</value>
     </property>
        <!-- 指定hadoop临时目录,自行创建 -->
     <property>
            <name>hadoop.tmp.dir</name>
            <value>/hadoop </value>
     </property>

    	```  
    		
    	- hdfs-site.xml
    	
    	```
    	<property>
      		<name>dfs.replication</name>
      		<value>2</value>
      </property>
		```	
    	
    	- **slaves:存储datanode的所有机器的hostname集群环境）**
	- 启动hdfs.  
		- 格式化文件系统（仅第一次执行）：bin/hdfs namenode -format    
		- 启动hdfs：sbin.start-dfs.sh   
		- 验证是否启动成功：jps、http://hadoop-1:50070   
			 
 5. HDFS shell.  
 	 - 查看文件：hadoop fs -ls /   
 	 - 上传文件 hadoop fs -put file /   
 	 - 查看文件内容：hadoop fs -text /file    hadoop fs -cat  /file   
 	 - 创建目录：hadoop fs -mkdir /test   
 	 - 递归创建目录: hadoop fs -mkdir -p /test/a/b   
 	 - 查看递归目录: hadoop fs -ls -R /   
 	 - 从本地拷贝: hadoop fs -copyFromLocal file /test/a/b/desfile   
 	 - 从文件系统拷贝:hadoop fs -get /test/a/b/desfile   
 	 - 删除文件系统文件:hadoop fs -rm file   
 	 - 删除文件系统目录:hadoop fs -rm -rf dir   

 6. Java API操作HDFS.  
 
    - 引入依赖(pom.xml)
 	 
	```
 	 <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <hadoop.version>2.8.4</hadoop.version>
    </properties>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>${hadoop.version}</version>
    </dependency>
    <repository>
        <id>cloudera</id>
        <url>https://repository.cloudera.com/artifactory/cloudera-repos</url>
    </repository>
    
	```
 	 
   - 测试示例
 
	```
  /**
     * 创建hdfs目录
     *
     * @throws IOException
     */
    @Test
    public void mkdir() throws IOException {
        fileSystem.mkdirs(new Path("/hdfs/api"));
    }

    /**
     * 创建文件
     *
     * @throws IOException
     */
    @Test
    public void create() throws IOException {
        FSDataOutputStream output = fileSystem.create(new Path("/hdfs/api/test.txt"));
        output.write("hello hadoop api".getBytes());
        output.flush();
        output.close();
    }

    /**
     * 查看HDFS内容
     *
     * @throws IOException
     */
    @Test
    public void cat() throws IOException {
        FSDataInputStream inputStream = fileSystem.open(new Path("/hdfs/api/test.txt"));
        IOUtils.copyBytes(inputStream, System.out, 1024);
        inputStream.close();
    }

    /**
     * 重命名HDFS文件
     *
     * @throws IOException
     */
    @Test
    public void rename() throws IOException {
        Path oldPath = new Path("/hdfs/api/test.txt");
        Path newPath = new Path("/hdfs/api/newtest.txt");
        fileSystem.rename(oldPath, newPath);
    }

    /**
     * 上传文件到HDFS
     *
     * @throws IOException
     */
    @Test
    public void copyFromLocalFile() throws IOException {
        Path localPath = new Path("/Users/zhanghaichao/hdu/1.cpp");
        Path hdfsPath = new Path("/hdfs/api/1.cpp");
        fileSystem.copyFromLocalFile(localPath, hdfsPath);
    }

    /**
     * 上传文件到HDFS,带进度条
     *
     * @throws IOException
     */
    @Test
    public void copyFromLocalFileWithProgress() throws IOException {
        Path hdfsPath = new Path("/hdfs/api/hadoop-2.8.4.tar.gz");

        InputStream inputStream = new BufferedInputStream(new FileInputStream(
                new File("/Users/zhanghaichao/Downloads/hadoop-2.8.4.tar.gz")));

        FSDataOutputStream outputStream = fileSystem.create(hdfsPath, new Progressable() {
            @Override
            public void progress() {
                System.out.println(".");
            }
        });
        IOUtils.copyBytes(inputStream, outputStream, 4096);

    }

    /**
     * 下载HDFS文件
     *
     * @throws IOException
     */
    @Test
    public void copyToLocalFile() throws IOException {
        Path localPath = new Path("/Users/zhanghaichao/1.cpp");
        Path hdfsPath = new Path("/hdfs/api/1.cpp");
        fileSystem.copyToLocalFile(hdfsPath, localPath);
    }

    /**
     * 查看HDFS目录下的所有文件
     *
     * @throws IOException
     */
    @Test
    public void listFiles() throws IOException {
        Path hdfsPath = new Path("/hdfs/api");
        FileStatus[] fileStatuses = fileSystem.listStatus(hdfsPath);
        for (FileStatus fileStatus : fileStatuses) {
            String isDir  = fileStatus.isDirectory() ? " is directory," : " is file,";
            short replication = fileStatus.getReplication(); 
            //hdfs shell 上传，副本系数使用配置文件中配置的，java api上传使用默认的（3）
            System.out.println(fileStatus.getPath().toString() + isDir
                    + "  replication is " + replication + "  length is :" + fileStatus.getLen());
        }
    }

    /**
     * 删除HDFS文件
     *
     * @throws IOException
     */
    @Test
    public void delete() throws IOException {
        Path hdfsPath = new Path("/fz");
        fileSystem.delete(hdfsPath,false); //默认递归删除，加参数控制
    }

    @Before
    public void setUp() throws URISyntaxException, IOException {
        configuration = new Configuration();
        fileSystem = FileSystem.get(new URI(HDFS_PATH), configuration);
        System.out.println("\n hdfs app setup\n");
    }

    @After
    public void tearDown() {
        configuration = null;
        fileSystem = null;

        System.out.println("\nhdfs app tear down\n");

    }
    
	```
   
 7. HDFS文件读写流程   
 	[通过漫画轻松掌握HDFS工作原理](https://blog.csdn.net/eric_sunah/article/details/41546863)
 8. HDFS优缺点
 